# ğŸ¤ Speech Emotion Recognition â€“ Modern CNNâ€¯&â€¯Spectrogram Pipeline ğŸš€

A stateâ€‘ofâ€‘theâ€‘art **speech emotion recognition** system that converts audio into spectrograms, analyzes them with a lightweight CNN, and serves realâ€‘time predictions via a Flask API. A fully **responsive React** frontend lets you upload clips and see live emotion inference.

---

## ğŸ› â€¯Tech Stack

### Backend  
- **Flask** for REST API endpoints
- **PyTorch** for model definition & inference 
- **Librosa** for audio loading & melâ€‘spectrogram extraction
- **scikitâ€‘learn**, **tqdm**, **wget** for data prep & utilities  

### Frontend  
- **React** (Createâ€¯Reactâ€¯App)  
- **Axios** for HTTP requests  
- **React Router**, **Bootstrapâ€¯4**, **AOS** for scroll animations  
- **reactâ€‘toastify** for notifications  

---

## ğŸ“š Dataset

We leverage the **RAVDESS** (Ryerson Audioâ€‘Visual Database of Emotional Speech and Song) datasetâ€”24 actors, 8 emotions. The backend autoâ€‘downloads & organizes it from Zenodo before training or inference. 

---

## ğŸ§  Algorithm & Pipeline

1. **Audio â†’ WAV**  
   Convert any input format to 16â€¯kHz WAV (viaâ€¯Librosa/Pydub).  
2. **Spectrogram Extraction**  
   Compute 13â€‘dimensional MFCC spectrograms (100â€‘frame fixed length).  
3. **CNN Classifier**  
   - **Conv1Dâ€¯Ã—â€¯3** (64â†’128â†’256 filters) + ReLU + MaxPool  
   - **Fusionâ€¯FC**: 3072â€¯â†’â€¯256 â†’â€¯7 classes  
4. **Training**  
   - Loss: CrossEntropy  
   - Optimizer: Adam (lrâ€¯=â€¯0.001)  
   - Epochs: 50  
5. **Inference**  
   - `/upload` endpoint returns one of seven emotions:  
     `angry`, `happy`, `sad`, `neutral`, `fearful`, `disgust`, `surprised`.

---

## ğŸ“‚ Project Structure

```
Speech-Emotion-Recognition/
â”œâ”€â”€ ML_Model/                   # ğŸ” Backend: data prep, model, Flask API
â”‚   â”œâ”€â”€ dataset/                # ğŸ§ Organized RAVDESS audio
â”‚   â”œâ”€â”€ emotion_recognition.py  # ğŸ¤– Model, dataset prep & train/predict
â”‚   â”œâ”€â”€ app.py                  # ğŸŒ Flask API server
â”‚   â””â”€â”€ best_model.pth          # ğŸ’¾ Trained weights
â””â”€â”€ speech-recognition/         # ğŸ’» React frontend (responsive SPA)
    â”œâ”€â”€ public/                 # ğŸŒ Static assets
    â”œâ”€â”€ src/                    # ğŸ§± Components, pages, styles
    â””â”€â”€ package.json            # ğŸ“¦ Dependencies & scripts
```

---

## ğŸš€ Quick Start

### 1. Clone & Prepare  
```bash
git clone https://github.com/Harry9021/Speech-Emotion-Recognition.git
cd Speech-Emotion-Recognition
```

### 2. Backend Setup  
```bash
cd ML_Model
python -m venv venv
# Windows: .\venv\Scripts\activate
# macOS/Linux: source venv/bin/activate

pip install flask torch torchvision librosa numpy wget tqdm scikit-learn
```

### 3. Frontend Setup  
```bash
cd ../speech-recognition
npm install
```

### 4. Run Everything  
- **Backend**:  
  ```bash
  cd ../ML_Model
  python app.py
  # â†’ http://localhost:5000
  ```
- **Frontend**:  
  ```bash
  cd ../speech-recognition
  npm start
  # â†’ http://localhost:3000
  ```

Upload an audio clip in the UI and watch the emotion appear! ğŸ‰

---

## ğŸ¤ Contributing

1. Fork & clone  
2. Create a branch (`git checkout -b feature-name`)  
3. Commit your changes (`git commit -m "feat: add new feature"`)  
4. Push & open a PR ğŸš€  

---

Made with â¤ï¸ by [@Harry9021](https://github.com/Harry9021)
